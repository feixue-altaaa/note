# 问题

容量不够，redis如何进行扩容？

并发写操作， redis如何分摊？

另外，主从模式，薪火相传模式，主机宕机，导致ip地址发生变化，应用程序中配置需要修改对应的主机地址、端口等信息

之前通过代理主机来解决，但是redis3.0中提供了解决方案。就是无中心化集群配置。

# 什么是集群

- Redis Cluster 集群模式具有 **高可用**、**可扩展性**、**分布式**、**容错** 等特性
- Redis 集群实现了对Redis的水平扩容，即启动N个redis节点，将整个数据库分布存储在这N个节点中，每个节点存储总数据的1/N
- Redis 集群通过分区（partition）来提供一定程度的可用性（availability）：即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求

# Redis的数据分布算法

## 哈希算法

哈希算法在分布式架构中应用广泛，不仅仅是数据存储，还有负载均衡等应用上有用的比较多，哈希算法的思想非常简单，也许你知道 HashMap 的哈希函数，哈希算法跟 HashMap 一样，也是通过一个哈希函数得到某一个数字，然后根据数字找到相应的服务器。哈希算法的哈希函数比较简单，一般是根据某个key的值或者key 的哈希值与当前可用的 master节点数取模，根据取模的值获取具体的服务器。哈希算法服务结构模型图如下图所示

![img](https://img-blog.csdnimg.cn/img_convert/751e0559ff36d234e84b8a75abe71be6.png)

**缺点**：哈希算法比较简单，可以看出只要你哈希函数设计的好，数据在各个服务器上是比较均匀分布的**，但是哈希算法有一个致命的缺点：**扩展性特别的差，比如我们的集群中，服务器server3 宕机了，这时候集群中可用的机器只有两台了，这样哈希函数就变成了`id % 2`了，这就会导致一个问题，所有的数据需要重新计算，找到新的存储节点，每次有服务器宕机或者添加机器时，都需要进行大量的数据迁移，这会使得系统的可用性、稳定性变差

## 一致性哈希算法

一致性哈希算法可以说是哈希算法的升级版，解决了哈希算法扩展性差的问题，一致性哈希算法跟哈希算法不一样，一致性哈希算法会将服务器和数据都通过哈希函数映射到一个首尾相连的哈希环上，存储节点映射可以根据 ip 地址来进行哈希取值，**数据映射到哈希环上后按照顺时针的方向查找存储节点，即从数据映射在环上的位置开始，顺时针方向找到的第一个存储节点，那么他就存储在这个节点上**。

我们使用一致性哈希算法来存储我们的数据，我画了一张图来模拟一致性哈希算法可能出现的结果

![img](https://img-blog.csdnimg.cn/img_convert/3bf23c8cdc6d3b68936165a4f542bf3f.png)

我们先来解读一下这张图，按照一致性哈希算法的规则，数据沿着顺时针的方向查找数据，那么 id=4 的数据存放在 server1 服务器，id=2 的数据存放在服务器 server2 上，id=3、id=1、id=5、id=6 的数据都存放在服务器 server3 上，如果你比较敏感的话，也许你就会发现一致性哈希算法的不足之处， 从图中可以看出，我们六条数据分布不均匀，并不是每台服务器存储 2 条数据，而且差距好像还有点大，这里我们就要来说一说一致性哈希算法的缺点：**一致性哈希算法会会造成数据分布不均匀的问题或者叫做数据倾斜问题**，就像我们图中那样，数据分布不均匀可能会造成某一个节点的负载过大，从而宕机。造成数据分布不均匀有以下两种情况：

- **第一**：哈希函数的原因，经过哈希函数之后服务器在哈希环上的分布不均匀，服务器之间的间距不相等，这样就会导致数据不均匀。
- **第二**：某服务器宕机了，后继节点就需要承受原本属于宕机机器的数据，这样也会造成数据不均匀。

前面我们提到过**一致性哈希算法解决了哈希算法中扩展性差的问题**，这个怎么理解呢？我们来看看，**在一致性哈希算法中当有存储节点加入或者退出时，只会影响应该该节点的后继节点**，举个例子说明一下，例如我们要在服务器server3 和服务 server2 之间加入了一个服务器存储节点 server4，只会对服务器server3 造成影响，原本存储到服务器server3 上的数据有一部分会落入到服务器 server4 上，对服务器 server1 和 server2 并没有任何影响，这样就不会进行大量的数据迁移，扩展性就变强

## 带有限负载的一致性哈希算法

因为一致性哈希算法的数据分布不均匀的问题，Google 在 2017 年提出了带有限负载的一致性哈希算法来解决这个问题，带有限负载的一致性哈希算法思想比较简单，**给每个存储节点设置了一个存储上限值来控制存储节点添加或移除造成的数据不均匀，当数据按照一致性哈希算法找到相应的存储节点时，要先判断该存储节点是否达到了存储上限；如果已经达到了上限，则需要继续寻找该存储节点顺时针方向之后的节点进行存储。**

我们利用带有限负载的一致性哈希算法来改进上面的数据存储，我们限定每台服务器节点存储的数据上限为 2 ，数据插入的顺序就按照 ID 大小的顺序，同样我也画了一张模拟图

![img](https://img-blog.csdnimg.cn/img_convert/9514428e8076e64d064d1e852f4de7da.png)

一起来分析一下这张图，因为我们的添加顺序是按照 id 大小的顺序，所以前四个数据都没有问题，这时候的服务器都没有超过最高负载数量，id=5 的数据落在了服务器server2 和服务器server3 之间，本应该是存储在服务器server3 上，但是由于此时的服务器server3 上已经存储了 id=1、id=3 的数据达到了最高限定，因此 id=5 的数据会沿着顺时针的方向继续往下寻找服务器，下一个服务器就是server1，此时的服务器server1 就存储了 id=4 的数据并没有达到上限，所以 id=5 的数据就会存储在服务器server1，id=6 的数据同样的道理。这样就利用带有限负载的一致性哈希算法解决了一致性哈希算法中数据分布不均匀的问题

## 带虚拟节点的一致性哈希算法

带有限负载的一致性哈希算法也有一个问题，那就是每台服务器的性能配置可能存在不一样，如果规定数量过小的话，对于配置高的服务器来说有点浪费，这是因为服务器之间可能存在差异，叫做服务器之间的异构性，为了解决服务器之间的异构性问题，引入了一种叫做**带虚拟节点的一致性哈希算法**，带虚拟节点的一致性哈希算法核心思想是：**根据每个节点的性能为每个节点划分不同数量的虚拟节点，并将这些虚拟节点映射到哈希环中，然后再按照一致性哈希算法进行数据映射和存储**。

为了演示带虚拟节点的一致性哈希算法，我们先做一个假设**服务器server3是配置最差的，所以我们以服务器server3 为基准，服务器server2 是服务器server3 的两倍，服务器server1 是服务器server3 的三倍**，有了这个前提之后，我们就可以来建立虚拟节点，我们假设服务器server3 的虚拟节点为服务器server3_1，服务器server2 就有两个虚拟节点服务器server2_1、服务器server2_2，服务器server1 有三个虚拟节点 服务器server1_1、服务器server1_2、服务器server1_3。我还是跟前面一样画了一张模拟图

![img](https://img-blog.csdnimg.cn/img_convert/2efbdf0786b3df1530c9c7814d5e8160.png)

落到虚拟节点上的数据都会存到对应的物理服务器上，所以通过带虚拟节点的一致性哈希算法后，数据存储结果为：数据id=2、id=3、id=5 的数据都会存储到服务器server1 上，id=1 的数据将会存储到服务器server2 上，数据 id=4、id=6 都会存放到服务器server3上。

虚拟节点可以让配置好的服务器存储更多的数据，这样就解决了系统异构性的问题，同时由于大量的虚拟节点的存在
在数据迁移时数据会落到不同的物理机上，这样就减小了数据迁移时某台服务器的分担压力，能够保证系统的稳定性

## 虚拟槽分区

虚拟槽分区是 redis cluster 中默认的数据分布技术，虚拟槽分区巧妙地使用了哈希空间，使用分散度良好的哈希函数把所有数据映射到一个固定范围的整数集合中，这个整数定义为槽（slot），而且这个槽的个数一般远远的大于节点数。

在 redis cluster 中有16384（0~16383）个槽，会将这些槽平均分配到每个 master 上，在存储数据时利用 CRC16 算法，具体的计算公式为：slot=CRC16（key）/16384 来计算 key 属于哪个槽。在我们的集群环境中，一个 key 的存储或者查找过程，可能如下图所示

![img](https://img-blog.csdnimg.cn/img_convert/49c5a1194afeadc0c3163c61d90026ed.png)

虚拟槽分区解耦了数据与节点的关系，通过引入槽，让槽成为集群内数据管理和迁移的基本单位，简化了节点扩容和收缩难度，你只需要关注数据在哪个槽，并不需要关心数据在哪个节点上。所以虚拟槽分区可以说比较好的兼容了数据均匀分布和扩展性的问题

# Redis三种集群模式原理

## 主从复制模式

### what

+ 主从复制模式是将多个节点中分为**主节点**和**从属节点**，通常主节点只有一台，从节点有很多台，主节点提供**写入**和**读取**功能，但是从属节点**只提供读取**功能

![image.png](https://raw.githubusercontent.com/feixue-altaaa/picture/master/pic/202307021536283.webp)

+ 图中`master`就是主节点，而`slave`就是从属节点，通常我们向主节点写入数据之后，主节点就会同步到从节点中去

### 优点

- 读写分离，职责明确
- 高可用，如果从属节点挂掉一个，则整个集群仍然可用

### 主从复制数据同步过程

- 首先主节点启动，然后从属节点启动，从属节点会连接主节点并发送`SYNC`命令以请求同步
- 主节点收到`SYNC`命令之后，就会执行`BGSAVE`命令生成`RDB`文件并**使用缓冲区记录此后执行的写入命令**
- 执行了`BGSAVE`之后，就向所有从属节点发送快照文件
- 从属节点收到快照文件之后，会**丢弃自己已有的所有旧数据**并把收到的快照写入数据库
- 之后，主节点会把缓冲区中的写命令发送给所有从节点实现从节点的**增量同步**

### 缺点

+ 主节点挂了，需要手动去设置一个从属节点变为主节点并修改其它节点配置

## 哨兵模式

### 原理

+ 哨兵模式事实上是上述主从复制模式的一种扩展，也就是说**在主从复制模式的基础上，增加哨兵节点以监视所有节点的情况**，假设主节点挂掉了，哨兵节点会从所有从属节点中**选举**一个节点作为新的主节点，并修改其余从属节点的配置

![image.png](https://raw.githubusercontent.com/feixue-altaaa/picture/master/pic/202307021539622.webp)

- **哨兵节点自身不会提供任何数据的读写存储功能**，仅仅是负责监视所有的节点。哨兵不仅会监视所有复制数据存储读写的主从节点，还会互相监视
- 通常哨兵节点也会有好几个，不过当其中一个哨兵监视到主节点挂掉时，系统并不会马上进行选举，因为这仅仅是一个哨兵看到主节点挂掉，这时称之为**主观下线**。而当其它哨兵也发现主节点挂掉时，并且发现主节点挂掉的哨兵达到一定值时，哨兵之间才会进行选举过程，这时称之为**客观下线**

### 哨兵模式工作过程

- 每个哨兵进程以**每秒钟一次**的频率向其它所有节点发送`PING`命令
- 如果一个节点距离最后一次有效回复时间间隔超过阈值，则这个节点会被这个哨兵标记为主观下线(`SDOWN`)，这个阈值可以通过哨兵节点配置文件中的`down-after-milliseconds`配置，单位ms
- 当主节点被任意一个哨兵标记为`SDOWN`，则正在监视主节点的所有哨兵会以每秒一次的频率确认主服务器的确进入了`SDOWN`状态
- 当有足够数量的哨兵在指定的时间范围内确认主节点进入了`SDOWN`，则主节点会被标记为客观下线(`ODOWN`)，进入故障切换(`failover`)过程开始选举，哨兵节点选举出新的主节点后，会以**发布订阅模式**通知其余从属节点修改配置文件指向新的主节点

**可见哨兵模式是一个自动化版的主从复制模式**

## Cluster模式

> **Redis Cluster 是不保证Redis高可用的，保证Redis高可用的是主从复制加哨兵模式**

### why

+ 在上述哨兵模式中，已经实现了高可用和读写分离。但是我们也可见每个节点都要储存一份完整的数据，这样很浪费内存。因此Redis官方推出了Cluster模式，这种模式下每个节点不会储存完整的内容，但是节点直接相互连通，所有节点内容加起来才是完整的内容

### 原理和过程

![image.png](https://raw.githubusercontent.com/feixue-altaaa/picture/master/pic/202307021542522.webp)

上述每个节点可能储存一部分内容，但是不论某个内容存放在哪个节点，我们都可以通过任意一个节点访问到，因为它们之间互相连通

那么，在整个集群中又是如何存放数据的呢？事实上，搭建集群的时候，Redis会根据节点数量先分配主从节点，然后根据主节点数量平均分配整个空间

Redis会先把整个集群所使用的储存空间分为**一定数量的等分**，这个等分就叫做**哈希槽(hash slot)**

Redis集群中有16384个哈希槽，那么假设集群中有三个主节点分别是A、B和C，每个主节点对应一个从属节点A1、B1和C1，那么主节点会被分配槽位如下

- A包含从`0-5460`哈希槽位
- B包含从`5461-10922`哈希槽位
- C包含从`10923-16383`哈希槽位

那我们存放数据时，这个数据到底会放到哪个槽位中去？事实上，Redis也有这样的一个算法：**存入数据时，就会对存入的键计算CRC16，然后拿计算出来的值对16384取模得到的结果，就是这个数据的槽位**

同样地，从属节点也会和上面一样同步主节点的内容。假设现在主节点B挂掉了，是不是意味着`5461-10922`槽位不能再储存数据了呢？当然不是。这时其对应的从属节点B1会被自动地提升为主节点
### Redis Cluster 为什么选哈希槽不选一致性哈希？

- **简化实现**：哈希槽的实现相对简单，不需要复杂的一致性哈希算法。这使得开发人员能够更快速地实现和维护集群
- **均匀分布，避免数据倾斜**：**Redis Cluster的槽位空间**是可以用户手动自定义分配的，类似于 windows 盘分区的概念，可以手动控制大小。哈希槽可以将数据均匀地分布到不同的节点上。每个节点负责一部分哈希槽，这样可以确保数据在整个集群中均匀分布，避免数据倾斜的问题
- **避免缓存雪崩**

### **为什么Redis中哈希槽的个数是16384？**

+ 理论上**CRC16算法**可以得到2的16次方个数值，其数值范围在0-65535之间，取模运算key的时候，应该是crc16(key)%65535，但是却设计为crc16(key)%16384，原因是作者在设计的时候做了空间上的权衡，觉得节点最多不可能超过1000个，同时为了保证节点之间通信效率，所以采用了2^14

+ 集群中的所有节点在握手成功后会定时发送 ping/pong 消息，交换数据信息

**消息体传递数据**

```java
typedef struct {
    //消息的长度（包括这个消息头的长度和消息正文的长度）
    uint32_t totlen;
    //消息的类型
    uint16_t type;
    //消息正文包含的节点信息数量
    //只在发送MEET 、PING 、PONG 这三种Gossip 协议消息时使用
    uint16_t count;
    //发送者所处的配置纪元
    uint64_t currentEpoch;
    //如果发送者是一个主节点，那么这里记录的是发送者的配置纪元
    //如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的配置纪元
    uint64_t configEpoch;
    //发送者的名字（ID ）
    char sender[REDIS_CLUSTER_NAMELEN];
    //发送者目前的槽指派信息
    unsigned char myslots[REDIS_CLUSTER_SLOTS/8];
    //如果发送者是一个从节点，那么这里记录的是发送者正在复制的主节点的名字
    //如果发送者是一个主节点，那么这里记录的是REDIS_NODE_NULL_NAME
    //（一个40 字节长，值全为0 的字节数组）
    char slaveof[REDIS_CLUSTER_NAMELEN];
    //发送者的端口号
    uint16_t port;
    //发送者的标识值
    uint16_t flags;
    //发送者所处集群的状态
    unsigned char state;
    //消息的正文（或者说，内容）
    union clusterMsgData data; 
} clusterMsg; 
```

+ **其中myslots 的 char 数组，长度为 16383/8**，这其实是一个 bitmap，每一个位代表一个槽，如果该位为1，表示这个槽是属于这个节点的
+ 至于这个消息体有多大？显然最占空间的就是 myslots 数组：`16384÷8÷1024=2kb`。如果槽位达到 65536，则所占空间提升到 `65536÷8÷1024=8kb`，极大浪费带宽
